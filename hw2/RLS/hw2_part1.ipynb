{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels import regression as sm\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'train.csv')\n",
    "test_df = pd.read_csv(r'test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "outputs": [],
   "source": [
    "# train_df.info()\n",
    "categorical = list(train_df.select_dtypes('object').columns)\n",
    "# categorical"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 1460 samples in the training data set and 80 features. There are 43 columns with the 'object' data type,\n",
    "meaning non-numeric categorical data. These features are contained in the \"categorical\" list. However, notice also that\n",
    "the 'MSSubClass' feature is numerical-categorical. Thus, there are actually 44 categorical features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will select seven non-categorical features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [],
   "source": [
    "# sns.pairplot(train_df[['SalePrice', 'LotArea', 'OverallQual', 'OverallCond', 'YearRemodAdd', 'FullBath', '1stFlrSF', '2ndFlrSF']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plots we care about here are in row 1 (or column 1). There appears to be a correlation between sales price and:\n",
    "Overall Quality, 1st Floor Area, 2nd Floor Area, and some slight correlations with Year of Remodelling, and\n",
    "Number of Full Baths."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "outputs": [],
   "source": [
    "# sm.linear_model.OLS()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% optional\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [],
   "source": [
    "MSSubClass_encoded = pd.get_dummies(train_df[['MSSubClass']].astype(str))\n",
    "train_df_dropped = train_df.drop('Id', axis=1)\n",
    "df_encoded = pd.get_dummies(train_df_dropped)\n",
    "df_encoded = pd.concat([df_encoded, MSSubClass_encoded], axis=1).drop('MSSubClass', axis=1)\n",
    "# df_encoded.info(verbose=True, null_counts=True)\n",
    "# df_encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Drop columns not useful to our model and perform one hot encoding.\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [],
   "source": [
    "split = ms.train_test_split(df_encoded, train_size=0.8)\n",
    "train_split = split[0]\n",
    "test_split = split[1]\n",
    "# train_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Splitting data\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "outputs": [],
   "source": [
    "# encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "# encoder.fit(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% sklearn's encoder (not used)\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "outputs": [
    {
     "data": {
      "text/plain": "['LotFrontage', 'MasVnrArea', 'GarageYrBlt']"
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_columns = df_encoded.columns\n",
    "columns = df_encoded.drop(['SalePrice'], axis=1).columns\n",
    "train_split.columns[train_split.isna().any()].tolist()\n",
    "# test_split.columns[test_split.isna().any()].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Look for columns with null values\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting values to the mean or zeroes could highly skew the results of a regression model.\n",
    "I will use KNN to perform multivariate imputation, filling in the above columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "outputs": [],
   "source": [
    "# Training the inputer with train split\n",
    "imputer_train = KNNImputer(n_neighbors=15, weights=\"uniform\")\n",
    "imputer_train.fit(train_split)\n",
    "train_split = pd.DataFrame(imputer_train.fit_transform(df_encoded), columns = all_columns)\n",
    "test_split = pd.DataFrame(imputer_train.fit_transform(test_split), columns = all_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% KNN inputation\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "outputs": [],
   "source": [
    "# training the normalizer with train split\n",
    "normalize_train = StandardScaler().fit(train_split.drop(['SalePrice'], axis=1))\n",
    "\n",
    "train_norm = normalize_train.transform(train_split.drop(['SalePrice'], axis=1))\n",
    "test_norm = normalize_train.transform(test_split.drop(['SalePrice'], axis=1))\n",
    "\n",
    "train_norm = pd.DataFrame(train_norm, columns = columns)\n",
    "test_norm = pd.DataFrame(test_norm, columns = columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Normalize the test/train data sets\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [
    "X = train_norm\n",
    "Y = train_split['SalePrice']\n",
    "X_test = test_norm\n",
    "Y_test = test_split['SalePrice']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9372192687370252"
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = lm.LinearRegression().fit(X, Y)\n",
    "lin_pred = lin_reg.predict(X_test)\n",
    "# lin_reg.score(test_norm.drop(['SalePrice'], axis=1), test_norm['SalePrice'])\n",
    "r2_score(Y_test, lin_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Predicting values and finding R^2 value\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 11}\n",
      "0.7405634361447293\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n0        0.003001      0.000448         0.004701        0.000458   \n1        0.002801      0.000600         0.005001        0.000632   \n2        0.002901      0.000300         0.005001        0.000001   \n3        0.003001      0.000633         0.005336        0.000447   \n4        0.002754      0.000518         0.005401        0.000490   \n5        0.003201      0.000600         0.005601        0.000490   \n6        0.002901      0.000300         0.005601        0.000490   \n7        0.003001      0.000447         0.005501        0.000500   \n8        0.002801      0.000400         0.005401        0.000664   \n9        0.003101      0.000300         0.005601        0.000490   \n10       0.003106      0.000299         0.005466        0.000835   \n11       0.003501      0.000671         0.005801        0.000400   \n12       0.002700      0.000458         0.005500        0.000672   \n13       0.002500      0.000500         0.005201        0.000600   \n\n   param_n_neighbors               params  split0_test_score  \\\n0                  1   {'n_neighbors': 1}           0.704337   \n1                  2   {'n_neighbors': 2}           0.760294   \n2                  3   {'n_neighbors': 3}           0.791678   \n3                  4   {'n_neighbors': 4}           0.779533   \n4                  5   {'n_neighbors': 5}           0.767779   \n5                  6   {'n_neighbors': 6}           0.773894   \n6                  7   {'n_neighbors': 7}           0.770276   \n7                  8   {'n_neighbors': 8}           0.810577   \n8                  9   {'n_neighbors': 9}           0.804290   \n9                 10  {'n_neighbors': 10}           0.799760   \n10                11  {'n_neighbors': 11}           0.802694   \n11                12  {'n_neighbors': 12}           0.798888   \n12                13  {'n_neighbors': 13}           0.794569   \n13                14  {'n_neighbors': 14}           0.796144   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0            0.288652           0.673225           0.668503   \n1            0.597509           0.785232           0.744155   \n2            0.680672           0.789750           0.732312   \n3            0.716608           0.789691           0.720356   \n4            0.721353           0.793127           0.690670   \n5            0.783611           0.795534           0.700564   \n6            0.769669           0.790530           0.676201   \n7            0.764780           0.791389           0.670147   \n8            0.769589           0.791150           0.670639   \n9            0.760243           0.791905           0.686254   \n10           0.752803           0.796325           0.691095   \n11           0.747025           0.793641           0.695943   \n12           0.738115           0.786389           0.686157   \n13           0.736029           0.784709           0.683200   \n\n    split4_test_score  split5_test_score  split6_test_score  \\\n0            0.728969           0.596297           0.750835   \n1            0.724942           0.711895           0.783782   \n2            0.732995           0.743530           0.791826   \n3            0.731071           0.745167           0.763657   \n4            0.733361           0.774366           0.749560   \n5            0.712149           0.785216           0.754382   \n6            0.700273           0.789617           0.766473   \n7            0.695494           0.779921           0.763272   \n8            0.694627           0.787157           0.760536   \n9            0.700491           0.779018           0.766560   \n10           0.704504           0.777661           0.775733   \n11           0.702116           0.763405           0.778144   \n12           0.700082           0.760412           0.777863   \n13           0.697719           0.756924           0.775142   \n\n    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n0            0.632051           0.345203           0.638231         0.602630   \n1            0.755437           0.457308           0.693007         0.701356   \n2            0.754361           0.515357           0.750465         0.728295   \n3            0.782534           0.535011           0.747364         0.731099   \n4            0.783860           0.549151           0.765257         0.732849   \n5            0.779214           0.547689           0.769856         0.740211   \n6            0.773471           0.543436           0.772189         0.735213   \n7            0.792394           0.561254           0.773031         0.740226   \n8            0.782024           0.558202           0.768746         0.738696   \n9            0.784111           0.552897           0.765767         0.738701   \n10           0.780312           0.557722           0.766785         0.740563   \n11           0.781128           0.569559           0.766188         0.739604   \n12           0.779301           0.567476           0.768658         0.735902   \n13           0.783376           0.565936           0.772331         0.735151   \n\n    std_test_score  rank_test_score  \n0         0.149873               14  \n1         0.096509               13  \n2         0.078027               12  \n3         0.069802               11  \n4         0.067945               10  \n5         0.070802                3  \n6         0.073298                8  \n7         0.072813                2  \n8         0.072640                6  \n9         0.071381                5  \n10        0.070127                1  \n11        0.065726                4  \n12        0.065908                7  \n13        0.066852                9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_n_neighbors</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.003001</td>\n      <td>0.000448</td>\n      <td>0.004701</td>\n      <td>0.000458</td>\n      <td>1</td>\n      <td>{'n_neighbors': 1}</td>\n      <td>0.704337</td>\n      <td>0.288652</td>\n      <td>0.673225</td>\n      <td>0.668503</td>\n      <td>0.728969</td>\n      <td>0.596297</td>\n      <td>0.750835</td>\n      <td>0.632051</td>\n      <td>0.345203</td>\n      <td>0.638231</td>\n      <td>0.602630</td>\n      <td>0.149873</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.002801</td>\n      <td>0.000600</td>\n      <td>0.005001</td>\n      <td>0.000632</td>\n      <td>2</td>\n      <td>{'n_neighbors': 2}</td>\n      <td>0.760294</td>\n      <td>0.597509</td>\n      <td>0.785232</td>\n      <td>0.744155</td>\n      <td>0.724942</td>\n      <td>0.711895</td>\n      <td>0.783782</td>\n      <td>0.755437</td>\n      <td>0.457308</td>\n      <td>0.693007</td>\n      <td>0.701356</td>\n      <td>0.096509</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.002901</td>\n      <td>0.000300</td>\n      <td>0.005001</td>\n      <td>0.000001</td>\n      <td>3</td>\n      <td>{'n_neighbors': 3}</td>\n      <td>0.791678</td>\n      <td>0.680672</td>\n      <td>0.789750</td>\n      <td>0.732312</td>\n      <td>0.732995</td>\n      <td>0.743530</td>\n      <td>0.791826</td>\n      <td>0.754361</td>\n      <td>0.515357</td>\n      <td>0.750465</td>\n      <td>0.728295</td>\n      <td>0.078027</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.003001</td>\n      <td>0.000633</td>\n      <td>0.005336</td>\n      <td>0.000447</td>\n      <td>4</td>\n      <td>{'n_neighbors': 4}</td>\n      <td>0.779533</td>\n      <td>0.716608</td>\n      <td>0.789691</td>\n      <td>0.720356</td>\n      <td>0.731071</td>\n      <td>0.745167</td>\n      <td>0.763657</td>\n      <td>0.782534</td>\n      <td>0.535011</td>\n      <td>0.747364</td>\n      <td>0.731099</td>\n      <td>0.069802</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002754</td>\n      <td>0.000518</td>\n      <td>0.005401</td>\n      <td>0.000490</td>\n      <td>5</td>\n      <td>{'n_neighbors': 5}</td>\n      <td>0.767779</td>\n      <td>0.721353</td>\n      <td>0.793127</td>\n      <td>0.690670</td>\n      <td>0.733361</td>\n      <td>0.774366</td>\n      <td>0.749560</td>\n      <td>0.783860</td>\n      <td>0.549151</td>\n      <td>0.765257</td>\n      <td>0.732849</td>\n      <td>0.067945</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.003201</td>\n      <td>0.000600</td>\n      <td>0.005601</td>\n      <td>0.000490</td>\n      <td>6</td>\n      <td>{'n_neighbors': 6}</td>\n      <td>0.773894</td>\n      <td>0.783611</td>\n      <td>0.795534</td>\n      <td>0.700564</td>\n      <td>0.712149</td>\n      <td>0.785216</td>\n      <td>0.754382</td>\n      <td>0.779214</td>\n      <td>0.547689</td>\n      <td>0.769856</td>\n      <td>0.740211</td>\n      <td>0.070802</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.002901</td>\n      <td>0.000300</td>\n      <td>0.005601</td>\n      <td>0.000490</td>\n      <td>7</td>\n      <td>{'n_neighbors': 7}</td>\n      <td>0.770276</td>\n      <td>0.769669</td>\n      <td>0.790530</td>\n      <td>0.676201</td>\n      <td>0.700273</td>\n      <td>0.789617</td>\n      <td>0.766473</td>\n      <td>0.773471</td>\n      <td>0.543436</td>\n      <td>0.772189</td>\n      <td>0.735213</td>\n      <td>0.073298</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.003001</td>\n      <td>0.000447</td>\n      <td>0.005501</td>\n      <td>0.000500</td>\n      <td>8</td>\n      <td>{'n_neighbors': 8}</td>\n      <td>0.810577</td>\n      <td>0.764780</td>\n      <td>0.791389</td>\n      <td>0.670147</td>\n      <td>0.695494</td>\n      <td>0.779921</td>\n      <td>0.763272</td>\n      <td>0.792394</td>\n      <td>0.561254</td>\n      <td>0.773031</td>\n      <td>0.740226</td>\n      <td>0.072813</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.002801</td>\n      <td>0.000400</td>\n      <td>0.005401</td>\n      <td>0.000664</td>\n      <td>9</td>\n      <td>{'n_neighbors': 9}</td>\n      <td>0.804290</td>\n      <td>0.769589</td>\n      <td>0.791150</td>\n      <td>0.670639</td>\n      <td>0.694627</td>\n      <td>0.787157</td>\n      <td>0.760536</td>\n      <td>0.782024</td>\n      <td>0.558202</td>\n      <td>0.768746</td>\n      <td>0.738696</td>\n      <td>0.072640</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.003101</td>\n      <td>0.000300</td>\n      <td>0.005601</td>\n      <td>0.000490</td>\n      <td>10</td>\n      <td>{'n_neighbors': 10}</td>\n      <td>0.799760</td>\n      <td>0.760243</td>\n      <td>0.791905</td>\n      <td>0.686254</td>\n      <td>0.700491</td>\n      <td>0.779018</td>\n      <td>0.766560</td>\n      <td>0.784111</td>\n      <td>0.552897</td>\n      <td>0.765767</td>\n      <td>0.738701</td>\n      <td>0.071381</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.003106</td>\n      <td>0.000299</td>\n      <td>0.005466</td>\n      <td>0.000835</td>\n      <td>11</td>\n      <td>{'n_neighbors': 11}</td>\n      <td>0.802694</td>\n      <td>0.752803</td>\n      <td>0.796325</td>\n      <td>0.691095</td>\n      <td>0.704504</td>\n      <td>0.777661</td>\n      <td>0.775733</td>\n      <td>0.780312</td>\n      <td>0.557722</td>\n      <td>0.766785</td>\n      <td>0.740563</td>\n      <td>0.070127</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.003501</td>\n      <td>0.000671</td>\n      <td>0.005801</td>\n      <td>0.000400</td>\n      <td>12</td>\n      <td>{'n_neighbors': 12}</td>\n      <td>0.798888</td>\n      <td>0.747025</td>\n      <td>0.793641</td>\n      <td>0.695943</td>\n      <td>0.702116</td>\n      <td>0.763405</td>\n      <td>0.778144</td>\n      <td>0.781128</td>\n      <td>0.569559</td>\n      <td>0.766188</td>\n      <td>0.739604</td>\n      <td>0.065726</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.002700</td>\n      <td>0.000458</td>\n      <td>0.005500</td>\n      <td>0.000672</td>\n      <td>13</td>\n      <td>{'n_neighbors': 13}</td>\n      <td>0.794569</td>\n      <td>0.738115</td>\n      <td>0.786389</td>\n      <td>0.686157</td>\n      <td>0.700082</td>\n      <td>0.760412</td>\n      <td>0.777863</td>\n      <td>0.779301</td>\n      <td>0.567476</td>\n      <td>0.768658</td>\n      <td>0.735902</td>\n      <td>0.065908</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.002500</td>\n      <td>0.000500</td>\n      <td>0.005201</td>\n      <td>0.000600</td>\n      <td>14</td>\n      <td>{'n_neighbors': 14}</td>\n      <td>0.796144</td>\n      <td>0.736029</td>\n      <td>0.784709</td>\n      <td>0.683200</td>\n      <td>0.697719</td>\n      <td>0.756924</td>\n      <td>0.775142</td>\n      <td>0.783376</td>\n      <td>0.565936</td>\n      <td>0.772331</td>\n      <td>0.735151</td>\n      <td>0.066852</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn_reg = KNeighborsRegressor()\n",
    "param_grid = {'n_neighbors': np.arange(1, 15)}\n",
    "knn_grid_cv = ms.GridSearchCV(knn_reg, param_grid, cv=10)\n",
    "knn_grid_cv.fit(X, Y)\n",
    "print(knn_grid_cv.best_params_)\n",
    "print(knn_grid_cv.best_score_)\n",
    "display(pd.DataFrame(knn_grid_cv.cv_results_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It appears that 11 neighbours gives us the best score. However, given the standard error of about 0.07, a score of at least\n",
    "0.67 performs similarly. Thus, the \"rule-of-thumb\" best selection could be argued to be k=2 KNN.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8505264801075464\n",
      "758.6209310344827\n"
     ]
    }
   ],
   "source": [
    "ridge_reg = lm.RidgeCV(alphas=np.linspace(0.001,1000,30), cv=10)\n",
    "ridge_reg.fit(X, Y)\n",
    "print(ridge_reg.best_score_)\n",
    "print(ridge_reg.alpha_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Using RidgeCV\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 758.6209310344827}\n",
      "0.8505264801075464\n"
     ]
    },
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0        0.007802  7.486405e-04         0.001601    4.901740e-04       0.001   \n1        0.007501  6.709140e-04         0.001901    3.001774e-04   34.483724   \n2        0.007902  2.999386e-04         0.001600    4.900475e-04   68.966448   \n3        0.007701  4.584951e-04         0.001901    3.003123e-04  103.449172   \n4        0.007687  4.507647e-04         0.002000    5.545475e-07  137.931897   \n5        0.007902  3.000182e-04         0.001800    4.001380e-04  172.414621   \n6        0.007596  4.964091e-04         0.002001    4.271620e-07  206.897345   \n7        0.007702  6.405602e-04         0.001901    2.999785e-04  241.380069   \n8        0.008002  6.326361e-04         0.001501    5.000355e-04  275.862793   \n9        0.009102  1.044202e-03         0.002100    3.000977e-04  310.345517   \n10       0.008502  8.064040e-04         0.001700    4.580062e-04  344.828241   \n11       0.008102  3.001140e-04         0.001801    4.003051e-04  379.310966   \n12       0.007702  4.582609e-04         0.002001    3.568323e-07   413.79369   \n13       0.007802  7.484876e-04         0.001700    4.583909e-04  448.276414   \n14       0.008002  4.475012e-04         0.001500    5.001546e-04  482.759138   \n15       0.007702  6.405342e-04         0.001900    3.001055e-04  517.241862   \n16       0.007902  5.385662e-04         0.002001    4.156970e-07  551.724586   \n17       0.008202  7.490802e-04         0.001800    4.000311e-04   586.20731   \n18       0.008402  6.637085e-04         0.001700    4.586825e-04  620.690034   \n19       0.009902  2.166196e-03         0.002300    4.583702e-04  655.172759   \n20       0.008002  4.523674e-07         0.001801    3.999711e-04  689.655483   \n21       0.007902  3.000424e-04         0.001901    3.000579e-04  724.138207   \n22       0.007702  4.583805e-04         0.001701    4.585418e-04  758.620931   \n23       0.007382  4.686024e-04         0.001720    4.751772e-04  793.103655   \n24       0.007902  5.386281e-04         0.001800    4.001023e-04  827.586379   \n25       0.007402  4.897458e-04         0.001800    4.001619e-04  862.069103   \n26       0.007802  4.000307e-04         0.001600    4.901643e-04  896.551828   \n27       0.007502  5.001309e-04         0.001700    4.582141e-04  931.034552   \n28       0.008302  1.005081e-03         0.001900    5.386149e-04  965.517276   \n29       0.008102  1.044234e-03         0.002301    4.584014e-04      1000.0   \n\n                           params  split0_test_score  split1_test_score  \\\n0                {'alpha': 0.001}           0.882797           0.824154   \n1   {'alpha': 34.483724137931034}           0.897533           0.830037   \n2    {'alpha': 68.96644827586208}           0.904408           0.836867   \n3   {'alpha': 103.44917241379312}           0.908695           0.842287   \n4   {'alpha': 137.93189655172415}           0.911698           0.846823   \n5   {'alpha': 172.41462068965518}           0.913944           0.850710   \n6   {'alpha': 206.89734482758624}           0.915691           0.854083   \n7   {'alpha': 241.38006896551727}           0.917084           0.857033   \n8   {'alpha': 275.86279310344827}           0.918214           0.859625   \n9    {'alpha': 310.3455172413793}           0.919140           0.861912   \n10   {'alpha': 344.8282413793103}           0.919903           0.863933   \n11  {'alpha': 379.31096551724136}           0.920534           0.865722   \n12  {'alpha': 413.79368965517244}           0.921055           0.867306   \n13   {'alpha': 448.2764137931035}           0.921484           0.868709   \n14   {'alpha': 482.7591379310345}           0.921833           0.869950   \n15   {'alpha': 517.2418620689655}           0.922114           0.871046   \n16   {'alpha': 551.7245862068966}           0.922336           0.872011   \n17   {'alpha': 586.2073103448276}           0.922507           0.872858   \n18   {'alpha': 620.6900344827586}           0.922631           0.873599   \n19   {'alpha': 655.1727586206897}           0.922714           0.874243   \n20   {'alpha': 689.6554827586207}           0.922761           0.874799   \n21   {'alpha': 724.1382068965518}           0.922775           0.875273   \n22   {'alpha': 758.6209310344827}           0.922760           0.875674   \n23   {'alpha': 793.1036551724138}           0.922717           0.876007   \n24   {'alpha': 827.5863793103449}           0.922650           0.876277   \n25   {'alpha': 862.0691034482759}           0.922561           0.876490   \n26    {'alpha': 896.551827586207}           0.922451           0.876650   \n27   {'alpha': 931.0345517241379}           0.922323           0.876760   \n28    {'alpha': 965.517275862069}           0.922176           0.876825   \n29              {'alpha': 1000.0}           0.922014           0.876847   \n\n    split2_test_score  split3_test_score  split4_test_score  \\\n0            0.912537           0.751018           0.900222   \n1            0.915897           0.761106           0.904970   \n2            0.916859           0.767384           0.904662   \n3            0.917150           0.771488           0.903451   \n4            0.917189           0.774447           0.901960   \n5            0.917125           0.776705           0.900367   \n6            0.917018           0.778492           0.898738   \n7            0.916891           0.779939           0.897101   \n8            0.916757           0.781132           0.895470   \n9            0.916620           0.782125           0.893852   \n10           0.916479           0.782958           0.892251   \n11           0.916336           0.783661           0.890669   \n12           0.916190           0.784255           0.889106   \n13           0.916039           0.784757           0.887562   \n14           0.915882           0.785179           0.886039   \n15           0.915718           0.785534           0.884535   \n16           0.915548           0.785829           0.883050   \n17           0.915369           0.786072           0.881584   \n18           0.915182           0.786268           0.880136   \n19           0.914987           0.786423           0.878706   \n20           0.914783           0.786541           0.877294   \n21           0.914570           0.786626           0.875898   \n22           0.914348           0.786681           0.874518   \n23           0.914117           0.786708           0.873154   \n24           0.913877           0.786710           0.871806   \n25           0.913627           0.786690           0.870472   \n26           0.913369           0.786649           0.869152   \n27           0.913103           0.786588           0.867847   \n28           0.912827           0.786510           0.866555   \n29           0.912544           0.786415           0.865276   \n\n    split5_test_score  split6_test_score  split7_test_score  \\\n0            0.589256           0.886591           0.895929   \n1            0.729670           0.890851           0.898326   \n2            0.759471           0.892386           0.898545   \n3            0.779878           0.893177           0.898464   \n4            0.795073           0.893558           0.898300   \n5            0.806892           0.893685           0.898104   \n6            0.816339           0.893641           0.897891   \n7            0.824037           0.893474           0.897666   \n8            0.830397           0.893215           0.897431   \n9            0.835706           0.892885           0.897186   \n10           0.840173           0.892500           0.896933   \n11           0.843954           0.892070           0.896670   \n12           0.847169           0.891604           0.896399   \n13           0.849909           0.891108           0.896119   \n14           0.852250           0.890587           0.895831   \n15           0.854251           0.890045           0.895535   \n16           0.855959           0.889485           0.895230   \n17           0.857416           0.888911           0.894918   \n18           0.858653           0.888323           0.894598   \n19           0.859700           0.887724           0.894271   \n20           0.860578           0.887116           0.893937   \n21           0.861308           0.886500           0.893596   \n22           0.861908           0.885876           0.893248   \n23           0.862391           0.885247           0.892894   \n24           0.862770           0.884612           0.892534   \n25           0.863056           0.883972           0.892167   \n26           0.863259           0.883329           0.891795   \n27           0.863387           0.882682           0.891417   \n28           0.863448           0.882032           0.891033   \n29           0.863448           0.881380           0.890644   \n\n    split8_test_score  split9_test_score  mean_test_score  std_test_score  \\\n0            0.420825          -1.254833         0.580850        0.630833   \n1            0.464817           0.900752         0.819396        0.133410   \n2            0.487943           0.899918         0.826844        0.125690   \n3            0.504712           0.899076         0.831838        0.120208   \n4            0.517851           0.898299         0.835520        0.115994   \n5            0.528586           0.897608         0.838373        0.112608   \n6            0.537597           0.897002         0.840649        0.109806   \n7            0.545305           0.896473         0.842500        0.107435   \n8            0.551995           0.896008         0.844024        0.105395   \n9            0.557867           0.895596         0.845289        0.103614   \n10           0.563070           0.895227         0.846343        0.102042   \n11           0.567716           0.894894         0.847223        0.100640   \n12           0.571891           0.894588         0.847956        0.099380   \n13           0.575665           0.894304         0.848566        0.098238   \n14           0.579093           0.894036         0.849068        0.097197   \n15           0.582220           0.893779         0.849478        0.096242   \n16           0.585082           0.893532         0.849806        0.095363   \n17           0.587712           0.893290         0.850064        0.094548   \n18           0.590135           0.893052         0.850258        0.093790   \n19           0.592373           0.892815         0.850396        0.093083   \n20           0.594445           0.892577         0.850483        0.092421   \n21           0.596368           0.892339         0.850525        0.091799   \n22           0.598155           0.892097         0.850526        0.091213   \n23           0.599820           0.891852         0.850491        0.090660   \n24           0.601372           0.891603         0.850421        0.090136   \n25           0.602821           0.891349         0.850321        0.089639   \n26           0.604176           0.891090         0.850192        0.089166   \n27           0.605444           0.890826         0.850038        0.088716   \n28           0.606632           0.890555         0.849859        0.088286   \n29           0.607745           0.890279         0.849659        0.087876   \n\n    rank_test_score  \n0                30  \n1                29  \n2                28  \n3                27  \n4                26  \n5                25  \n6                24  \n7                23  \n8                22  \n9                21  \n10               20  \n11               19  \n12               18  \n13               17  \n14               16  \n15               15  \n16               13  \n17               10  \n18                8  \n19                6  \n20                4  \n21                2  \n22                1  \n23                3  \n24                5  \n25                7  \n26                9  \n27               11  \n28               12  \n29               14  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>split5_test_score</th>\n      <th>split6_test_score</th>\n      <th>split7_test_score</th>\n      <th>split8_test_score</th>\n      <th>split9_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007802</td>\n      <td>7.486405e-04</td>\n      <td>0.001601</td>\n      <td>4.901740e-04</td>\n      <td>0.001</td>\n      <td>{'alpha': 0.001}</td>\n      <td>0.882797</td>\n      <td>0.824154</td>\n      <td>0.912537</td>\n      <td>0.751018</td>\n      <td>0.900222</td>\n      <td>0.589256</td>\n      <td>0.886591</td>\n      <td>0.895929</td>\n      <td>0.420825</td>\n      <td>-1.254833</td>\n      <td>0.580850</td>\n      <td>0.630833</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.007501</td>\n      <td>6.709140e-04</td>\n      <td>0.001901</td>\n      <td>3.001774e-04</td>\n      <td>34.483724</td>\n      <td>{'alpha': 34.483724137931034}</td>\n      <td>0.897533</td>\n      <td>0.830037</td>\n      <td>0.915897</td>\n      <td>0.761106</td>\n      <td>0.904970</td>\n      <td>0.729670</td>\n      <td>0.890851</td>\n      <td>0.898326</td>\n      <td>0.464817</td>\n      <td>0.900752</td>\n      <td>0.819396</td>\n      <td>0.133410</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.007902</td>\n      <td>2.999386e-04</td>\n      <td>0.001600</td>\n      <td>4.900475e-04</td>\n      <td>68.966448</td>\n      <td>{'alpha': 68.96644827586208}</td>\n      <td>0.904408</td>\n      <td>0.836867</td>\n      <td>0.916859</td>\n      <td>0.767384</td>\n      <td>0.904662</td>\n      <td>0.759471</td>\n      <td>0.892386</td>\n      <td>0.898545</td>\n      <td>0.487943</td>\n      <td>0.899918</td>\n      <td>0.826844</td>\n      <td>0.125690</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.007701</td>\n      <td>4.584951e-04</td>\n      <td>0.001901</td>\n      <td>3.003123e-04</td>\n      <td>103.449172</td>\n      <td>{'alpha': 103.44917241379312}</td>\n      <td>0.908695</td>\n      <td>0.842287</td>\n      <td>0.917150</td>\n      <td>0.771488</td>\n      <td>0.903451</td>\n      <td>0.779878</td>\n      <td>0.893177</td>\n      <td>0.898464</td>\n      <td>0.504712</td>\n      <td>0.899076</td>\n      <td>0.831838</td>\n      <td>0.120208</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.007687</td>\n      <td>4.507647e-04</td>\n      <td>0.002000</td>\n      <td>5.545475e-07</td>\n      <td>137.931897</td>\n      <td>{'alpha': 137.93189655172415}</td>\n      <td>0.911698</td>\n      <td>0.846823</td>\n      <td>0.917189</td>\n      <td>0.774447</td>\n      <td>0.901960</td>\n      <td>0.795073</td>\n      <td>0.893558</td>\n      <td>0.898300</td>\n      <td>0.517851</td>\n      <td>0.898299</td>\n      <td>0.835520</td>\n      <td>0.115994</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.007902</td>\n      <td>3.000182e-04</td>\n      <td>0.001800</td>\n      <td>4.001380e-04</td>\n      <td>172.414621</td>\n      <td>{'alpha': 172.41462068965518}</td>\n      <td>0.913944</td>\n      <td>0.850710</td>\n      <td>0.917125</td>\n      <td>0.776705</td>\n      <td>0.900367</td>\n      <td>0.806892</td>\n      <td>0.893685</td>\n      <td>0.898104</td>\n      <td>0.528586</td>\n      <td>0.897608</td>\n      <td>0.838373</td>\n      <td>0.112608</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.007596</td>\n      <td>4.964091e-04</td>\n      <td>0.002001</td>\n      <td>4.271620e-07</td>\n      <td>206.897345</td>\n      <td>{'alpha': 206.89734482758624}</td>\n      <td>0.915691</td>\n      <td>0.854083</td>\n      <td>0.917018</td>\n      <td>0.778492</td>\n      <td>0.898738</td>\n      <td>0.816339</td>\n      <td>0.893641</td>\n      <td>0.897891</td>\n      <td>0.537597</td>\n      <td>0.897002</td>\n      <td>0.840649</td>\n      <td>0.109806</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.007702</td>\n      <td>6.405602e-04</td>\n      <td>0.001901</td>\n      <td>2.999785e-04</td>\n      <td>241.380069</td>\n      <td>{'alpha': 241.38006896551727}</td>\n      <td>0.917084</td>\n      <td>0.857033</td>\n      <td>0.916891</td>\n      <td>0.779939</td>\n      <td>0.897101</td>\n      <td>0.824037</td>\n      <td>0.893474</td>\n      <td>0.897666</td>\n      <td>0.545305</td>\n      <td>0.896473</td>\n      <td>0.842500</td>\n      <td>0.107435</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.008002</td>\n      <td>6.326361e-04</td>\n      <td>0.001501</td>\n      <td>5.000355e-04</td>\n      <td>275.862793</td>\n      <td>{'alpha': 275.86279310344827}</td>\n      <td>0.918214</td>\n      <td>0.859625</td>\n      <td>0.916757</td>\n      <td>0.781132</td>\n      <td>0.895470</td>\n      <td>0.830397</td>\n      <td>0.893215</td>\n      <td>0.897431</td>\n      <td>0.551995</td>\n      <td>0.896008</td>\n      <td>0.844024</td>\n      <td>0.105395</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.009102</td>\n      <td>1.044202e-03</td>\n      <td>0.002100</td>\n      <td>3.000977e-04</td>\n      <td>310.345517</td>\n      <td>{'alpha': 310.3455172413793}</td>\n      <td>0.919140</td>\n      <td>0.861912</td>\n      <td>0.916620</td>\n      <td>0.782125</td>\n      <td>0.893852</td>\n      <td>0.835706</td>\n      <td>0.892885</td>\n      <td>0.897186</td>\n      <td>0.557867</td>\n      <td>0.895596</td>\n      <td>0.845289</td>\n      <td>0.103614</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.008502</td>\n      <td>8.064040e-04</td>\n      <td>0.001700</td>\n      <td>4.580062e-04</td>\n      <td>344.828241</td>\n      <td>{'alpha': 344.8282413793103}</td>\n      <td>0.919903</td>\n      <td>0.863933</td>\n      <td>0.916479</td>\n      <td>0.782958</td>\n      <td>0.892251</td>\n      <td>0.840173</td>\n      <td>0.892500</td>\n      <td>0.896933</td>\n      <td>0.563070</td>\n      <td>0.895227</td>\n      <td>0.846343</td>\n      <td>0.102042</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.008102</td>\n      <td>3.001140e-04</td>\n      <td>0.001801</td>\n      <td>4.003051e-04</td>\n      <td>379.310966</td>\n      <td>{'alpha': 379.31096551724136}</td>\n      <td>0.920534</td>\n      <td>0.865722</td>\n      <td>0.916336</td>\n      <td>0.783661</td>\n      <td>0.890669</td>\n      <td>0.843954</td>\n      <td>0.892070</td>\n      <td>0.896670</td>\n      <td>0.567716</td>\n      <td>0.894894</td>\n      <td>0.847223</td>\n      <td>0.100640</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.007702</td>\n      <td>4.582609e-04</td>\n      <td>0.002001</td>\n      <td>3.568323e-07</td>\n      <td>413.79369</td>\n      <td>{'alpha': 413.79368965517244}</td>\n      <td>0.921055</td>\n      <td>0.867306</td>\n      <td>0.916190</td>\n      <td>0.784255</td>\n      <td>0.889106</td>\n      <td>0.847169</td>\n      <td>0.891604</td>\n      <td>0.896399</td>\n      <td>0.571891</td>\n      <td>0.894588</td>\n      <td>0.847956</td>\n      <td>0.099380</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.007802</td>\n      <td>7.484876e-04</td>\n      <td>0.001700</td>\n      <td>4.583909e-04</td>\n      <td>448.276414</td>\n      <td>{'alpha': 448.2764137931035}</td>\n      <td>0.921484</td>\n      <td>0.868709</td>\n      <td>0.916039</td>\n      <td>0.784757</td>\n      <td>0.887562</td>\n      <td>0.849909</td>\n      <td>0.891108</td>\n      <td>0.896119</td>\n      <td>0.575665</td>\n      <td>0.894304</td>\n      <td>0.848566</td>\n      <td>0.098238</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.008002</td>\n      <td>4.475012e-04</td>\n      <td>0.001500</td>\n      <td>5.001546e-04</td>\n      <td>482.759138</td>\n      <td>{'alpha': 482.7591379310345}</td>\n      <td>0.921833</td>\n      <td>0.869950</td>\n      <td>0.915882</td>\n      <td>0.785179</td>\n      <td>0.886039</td>\n      <td>0.852250</td>\n      <td>0.890587</td>\n      <td>0.895831</td>\n      <td>0.579093</td>\n      <td>0.894036</td>\n      <td>0.849068</td>\n      <td>0.097197</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.007702</td>\n      <td>6.405342e-04</td>\n      <td>0.001900</td>\n      <td>3.001055e-04</td>\n      <td>517.241862</td>\n      <td>{'alpha': 517.2418620689655}</td>\n      <td>0.922114</td>\n      <td>0.871046</td>\n      <td>0.915718</td>\n      <td>0.785534</td>\n      <td>0.884535</td>\n      <td>0.854251</td>\n      <td>0.890045</td>\n      <td>0.895535</td>\n      <td>0.582220</td>\n      <td>0.893779</td>\n      <td>0.849478</td>\n      <td>0.096242</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.007902</td>\n      <td>5.385662e-04</td>\n      <td>0.002001</td>\n      <td>4.156970e-07</td>\n      <td>551.724586</td>\n      <td>{'alpha': 551.7245862068966}</td>\n      <td>0.922336</td>\n      <td>0.872011</td>\n      <td>0.915548</td>\n      <td>0.785829</td>\n      <td>0.883050</td>\n      <td>0.855959</td>\n      <td>0.889485</td>\n      <td>0.895230</td>\n      <td>0.585082</td>\n      <td>0.893532</td>\n      <td>0.849806</td>\n      <td>0.095363</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.008202</td>\n      <td>7.490802e-04</td>\n      <td>0.001800</td>\n      <td>4.000311e-04</td>\n      <td>586.20731</td>\n      <td>{'alpha': 586.2073103448276}</td>\n      <td>0.922507</td>\n      <td>0.872858</td>\n      <td>0.915369</td>\n      <td>0.786072</td>\n      <td>0.881584</td>\n      <td>0.857416</td>\n      <td>0.888911</td>\n      <td>0.894918</td>\n      <td>0.587712</td>\n      <td>0.893290</td>\n      <td>0.850064</td>\n      <td>0.094548</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.008402</td>\n      <td>6.637085e-04</td>\n      <td>0.001700</td>\n      <td>4.586825e-04</td>\n      <td>620.690034</td>\n      <td>{'alpha': 620.6900344827586}</td>\n      <td>0.922631</td>\n      <td>0.873599</td>\n      <td>0.915182</td>\n      <td>0.786268</td>\n      <td>0.880136</td>\n      <td>0.858653</td>\n      <td>0.888323</td>\n      <td>0.894598</td>\n      <td>0.590135</td>\n      <td>0.893052</td>\n      <td>0.850258</td>\n      <td>0.093790</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.009902</td>\n      <td>2.166196e-03</td>\n      <td>0.002300</td>\n      <td>4.583702e-04</td>\n      <td>655.172759</td>\n      <td>{'alpha': 655.1727586206897}</td>\n      <td>0.922714</td>\n      <td>0.874243</td>\n      <td>0.914987</td>\n      <td>0.786423</td>\n      <td>0.878706</td>\n      <td>0.859700</td>\n      <td>0.887724</td>\n      <td>0.894271</td>\n      <td>0.592373</td>\n      <td>0.892815</td>\n      <td>0.850396</td>\n      <td>0.093083</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.008002</td>\n      <td>4.523674e-07</td>\n      <td>0.001801</td>\n      <td>3.999711e-04</td>\n      <td>689.655483</td>\n      <td>{'alpha': 689.6554827586207}</td>\n      <td>0.922761</td>\n      <td>0.874799</td>\n      <td>0.914783</td>\n      <td>0.786541</td>\n      <td>0.877294</td>\n      <td>0.860578</td>\n      <td>0.887116</td>\n      <td>0.893937</td>\n      <td>0.594445</td>\n      <td>0.892577</td>\n      <td>0.850483</td>\n      <td>0.092421</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.007902</td>\n      <td>3.000424e-04</td>\n      <td>0.001901</td>\n      <td>3.000579e-04</td>\n      <td>724.138207</td>\n      <td>{'alpha': 724.1382068965518}</td>\n      <td>0.922775</td>\n      <td>0.875273</td>\n      <td>0.914570</td>\n      <td>0.786626</td>\n      <td>0.875898</td>\n      <td>0.861308</td>\n      <td>0.886500</td>\n      <td>0.893596</td>\n      <td>0.596368</td>\n      <td>0.892339</td>\n      <td>0.850525</td>\n      <td>0.091799</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.007702</td>\n      <td>4.583805e-04</td>\n      <td>0.001701</td>\n      <td>4.585418e-04</td>\n      <td>758.620931</td>\n      <td>{'alpha': 758.6209310344827}</td>\n      <td>0.922760</td>\n      <td>0.875674</td>\n      <td>0.914348</td>\n      <td>0.786681</td>\n      <td>0.874518</td>\n      <td>0.861908</td>\n      <td>0.885876</td>\n      <td>0.893248</td>\n      <td>0.598155</td>\n      <td>0.892097</td>\n      <td>0.850526</td>\n      <td>0.091213</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.007382</td>\n      <td>4.686024e-04</td>\n      <td>0.001720</td>\n      <td>4.751772e-04</td>\n      <td>793.103655</td>\n      <td>{'alpha': 793.1036551724138}</td>\n      <td>0.922717</td>\n      <td>0.876007</td>\n      <td>0.914117</td>\n      <td>0.786708</td>\n      <td>0.873154</td>\n      <td>0.862391</td>\n      <td>0.885247</td>\n      <td>0.892894</td>\n      <td>0.599820</td>\n      <td>0.891852</td>\n      <td>0.850491</td>\n      <td>0.090660</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.007902</td>\n      <td>5.386281e-04</td>\n      <td>0.001800</td>\n      <td>4.001023e-04</td>\n      <td>827.586379</td>\n      <td>{'alpha': 827.5863793103449}</td>\n      <td>0.922650</td>\n      <td>0.876277</td>\n      <td>0.913877</td>\n      <td>0.786710</td>\n      <td>0.871806</td>\n      <td>0.862770</td>\n      <td>0.884612</td>\n      <td>0.892534</td>\n      <td>0.601372</td>\n      <td>0.891603</td>\n      <td>0.850421</td>\n      <td>0.090136</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.007402</td>\n      <td>4.897458e-04</td>\n      <td>0.001800</td>\n      <td>4.001619e-04</td>\n      <td>862.069103</td>\n      <td>{'alpha': 862.0691034482759}</td>\n      <td>0.922561</td>\n      <td>0.876490</td>\n      <td>0.913627</td>\n      <td>0.786690</td>\n      <td>0.870472</td>\n      <td>0.863056</td>\n      <td>0.883972</td>\n      <td>0.892167</td>\n      <td>0.602821</td>\n      <td>0.891349</td>\n      <td>0.850321</td>\n      <td>0.089639</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.007802</td>\n      <td>4.000307e-04</td>\n      <td>0.001600</td>\n      <td>4.901643e-04</td>\n      <td>896.551828</td>\n      <td>{'alpha': 896.551827586207}</td>\n      <td>0.922451</td>\n      <td>0.876650</td>\n      <td>0.913369</td>\n      <td>0.786649</td>\n      <td>0.869152</td>\n      <td>0.863259</td>\n      <td>0.883329</td>\n      <td>0.891795</td>\n      <td>0.604176</td>\n      <td>0.891090</td>\n      <td>0.850192</td>\n      <td>0.089166</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.007502</td>\n      <td>5.001309e-04</td>\n      <td>0.001700</td>\n      <td>4.582141e-04</td>\n      <td>931.034552</td>\n      <td>{'alpha': 931.0345517241379}</td>\n      <td>0.922323</td>\n      <td>0.876760</td>\n      <td>0.913103</td>\n      <td>0.786588</td>\n      <td>0.867847</td>\n      <td>0.863387</td>\n      <td>0.882682</td>\n      <td>0.891417</td>\n      <td>0.605444</td>\n      <td>0.890826</td>\n      <td>0.850038</td>\n      <td>0.088716</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.008302</td>\n      <td>1.005081e-03</td>\n      <td>0.001900</td>\n      <td>5.386149e-04</td>\n      <td>965.517276</td>\n      <td>{'alpha': 965.517275862069}</td>\n      <td>0.922176</td>\n      <td>0.876825</td>\n      <td>0.912827</td>\n      <td>0.786510</td>\n      <td>0.866555</td>\n      <td>0.863448</td>\n      <td>0.882032</td>\n      <td>0.891033</td>\n      <td>0.606632</td>\n      <td>0.890555</td>\n      <td>0.849859</td>\n      <td>0.088286</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.008102</td>\n      <td>1.044234e-03</td>\n      <td>0.002301</td>\n      <td>4.584014e-04</td>\n      <td>1000.0</td>\n      <td>{'alpha': 1000.0}</td>\n      <td>0.922014</td>\n      <td>0.876847</td>\n      <td>0.912544</td>\n      <td>0.786415</td>\n      <td>0.865276</td>\n      <td>0.863448</td>\n      <td>0.881380</td>\n      <td>0.890644</td>\n      <td>0.607745</td>\n      <td>0.890279</td>\n      <td>0.849659</td>\n      <td>0.087876</td>\n      <td>14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ridge_reg = lm.Ridge()\n",
    "param_grid = {'alpha': np.linspace(0.001,1000,30)}\n",
    "ridge_grid_cv = ms.GridSearchCV(ridge_reg, param_grid, cv=10)\n",
    "ridge_grid_cv.fit(X, Y)\n",
    "print(ridge_grid_cv.best_params_)\n",
    "print(ridge_grid_cv.best_score_)\n",
    "display(pd.DataFrame(ridge_grid_cv.cv_results_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Using GridSearch CV\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}